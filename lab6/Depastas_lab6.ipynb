{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Lab5_RBM_DBN.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1><b>Restricted Boltzmann Machine και Deep Belief Network</b></h1>\n",
    "<p align=\"justify\">Στην συγκεκριμένη άσκηση θα μελετήσετε τον τρόπο λειτουργίας μιας <i>RBM (<a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">Restricted Boltzmann Machine</a>)</i> καθώς και των <i>DBN (<a href=\"https://en.wikipedia.org/wiki/Deep_belief_network\">Deep Belief Network</a>)</i>, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.Το συγκεκριμένο πρόγραμμα αξιοποιεί το <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">dataset του <i>MNIST</i></a>, όπου είναι μια μεγάλη βάση δεδομένων με χειρόγραφα ψηφία που χρησιμοποιείται συνήθως για την εκπαίδευση διαφόρων συστημάτων επεξεργασίας εικόνας. Για την άσκηση, θα πρέπει να χρησιμοποιήσετε το αρχείο <i>mnist_original.mat</i>, το οποίο είναι διαθέσιμο από <a href=\"https://www.kaggle.com/datasets/avnishnish/mnist-original?resource=download\">εδώ</a>.</p>\n",
    "<p align=\"justify\">Μία αρκετά σημαντική εφαρμογή της <i>RBM</i> είναι η εξαγωγή χαρακτηριστικών (feature representation) από ένα dataset με σκοπό την αναπαράσταση της εισόδου (ορατοί νευρώνες) με ένα διάνυσμα μικρότερης διάστασης (κρυφοί νευρώνες). Στη συγκεκριμένη άσκηση θα συγκρίνετε την ακρίβεια ενός ταξινομητή ψηφίων με τη χρήση του αλγορίθμου <i>Logistic Regression</i>, όταν εκείνος δέχεται ως είσοδο το dataset (i) χωρίς να έχει υποστεί επεξεργασία από το <i>RBM</i>, (ii) αφου υποστεί επεξεργασία από το <i>RBM</i>, (iii) με τη χρήση <i>DBN</i>, δηλαδή δύο stacked <i>RBM</i>.</p>\n",
    "<p align=\"justify\"> Με βάση τον κώδικα που σας έχει δοθεί, καλείστε να απαντήσετε στα παρακάτω ερωτήματα:</p>\n",
    "<ul>\n",
    "<li>Να περιγράψετε σύντομα τον τρόπο λειτουργίας μιας <i>RBM</i>. Τι διαφορές έχει σε σχέση με μία <i> Μηχανή Boltzmann</i>;</li>\n",
    "<li>Ποια είναι η λογική των <i>DBN</i> και σε τι προβλήματα τα αξιοποιούμε;</li>\n",
    "<li>Να αναφέρετε τις βασικότερες εφαρμογές των <i>RBM</i> και <i>DBN</i>.</li>\n",
    "<li>Εκτός από <i>RBM</i>, τι άλλα μοντέλα μπορούν να χρησιμοποιηθούν για να δημιουργήσουν <i>DBN</i>.</li>\n",
    "<li>Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo <i>Logistic Regression</i> χωρίς τη χρήση <i>RBM</i> σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η <i>RBM</i> καθώς και με αυτή όπου χρησιμοποιούνται <i>RBM</i> και <i>DBN</i> για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;</li>\n",
    "</ul>"
   ],
   "metadata": {
    "id": "B5q7C447X8Up"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "# source: https://devdreamz.com/question/905929-stacking-rbms-to-create-deep-belief-network-in-sklearn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def norm(arr):\n",
    "    arr = arr.astype(float)\n",
    "    arr -= arr.min()\n",
    "    arr /= arr.max()\n",
    "    return arr\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # load MNIST data set\n",
    "    mnist = loadmat(\"mnist-original.mat\")\n",
    "    X, Y = mnist[\"data\"].T, mnist[\"label\"][0]\n",
    "\n",
    "    # normalize inputs to 0-1 range\n",
    "    X = norm(X)\n",
    "\n",
    "    # split into train, validation, and test data sets\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,       Y,       test_size=10000, random_state=0)\n",
    "    X_train, X_val,  Y_train, Y_val  = train_test_split(X_train, Y_train, test_size=10000, random_state=0)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # set hyperparameters\n",
    "\n",
    "    learning_rate = 0.02 \n",
    "    total_units   =  800 \n",
    "    total_epochs  =   50\n",
    "    batch_size    =  128\n",
    "\n",
    "    C = 100. # optimum for benchmark model according to sklearn docs: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py)\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # construct models\n",
    "\n",
    "    # RBM\n",
    "    rbm = BernoulliRBM(n_components=total_units, learning_rate=learning_rate, batch_size=batch_size, n_iter=total_epochs, verbose=1)\n",
    "\n",
    "    # \"output layer\"\n",
    "    logistic = LogisticRegression(C=C, solver='lbfgs', multi_class='multinomial', max_iter=200, verbose=1)\n",
    "\n",
    "    models = []\n",
    "    models.append(Pipeline(steps=[('logistic', clone(logistic))]))                                              # base model / benchmark\n",
    "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('logistic', clone(logistic))]))                        # single RBM\n",
    "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('rbm2', clone(rbm)), ('logistic', clone(logistic))]))  # RBM stack / DBN\n",
    "\n",
    "    # --------------------------------------------------------------------------------\n",
    "    # train and evaluate models\n",
    "\n",
    "    for model in models:\n",
    "        # train\n",
    "        model.fit(X_train, Y_train)\n",
    "\n",
    "        # evaluate using validation set\n",
    "        print(\"Model performance:\\n%s\\n\" % (\n",
    "            classification_report(Y_val, model.predict(X_val))))"
   ],
   "metadata": {
    "id": "Z6OQ6-N8ajZJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\User\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.96       995\n",
      "         1.0       0.96      0.98      0.97      1121\n",
      "         2.0       0.90      0.90      0.90      1015\n",
      "         3.0       0.90      0.88      0.89      1033\n",
      "         4.0       0.93      0.92      0.92       976\n",
      "         5.0       0.90      0.88      0.89       884\n",
      "         6.0       0.94      0.94      0.94       999\n",
      "         7.0       0.92      0.93      0.92      1034\n",
      "         8.0       0.88      0.87      0.87       923\n",
      "         9.0       0.89      0.90      0.90      1020\n",
      "\n",
      "    accuracy                           0.92     10000\n",
      "   macro avg       0.92      0.92      0.92     10000\n",
      "weighted avg       0.92      0.92      0.92     10000\n",
      "\n",
      "\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -139.70, time = 27.41s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -117.34, time = 36.58s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -105.13, time = 32.35s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -97.63, time = 38.88s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -94.98, time = 31.80s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -87.86, time = 39.29s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -86.79, time = 37.53s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -81.43, time = 38.10s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -81.88, time = 38.08s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -77.76, time = 36.26s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -78.80, time = 35.84s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -75.92, time = 34.25s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -74.99, time = 30.20s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -72.85, time = 31.79s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -71.80, time = 30.57s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -71.79, time = 30.36s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -71.96, time = 40.60s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -71.06, time = 36.15s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -70.25, time = 33.74s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -68.96, time = 38.88s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -68.86, time = 29.32s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -68.93, time = 35.97s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -68.43, time = 34.48s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -67.83, time = 38.52s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -66.51, time = 39.38s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -65.56, time = 31.46s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -66.39, time = 35.60s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -65.94, time = 35.33s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -64.38, time = 37.23s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -65.33, time = 29.04s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -63.82, time = 33.78s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -64.99, time = 31.91s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -63.60, time = 28.82s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -63.90, time = 30.45s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -64.99, time = 26.18s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -63.45, time = 28.35s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -64.62, time = 31.81s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -64.17, time = 26.11s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -64.22, time = 40.18s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -62.16, time = 31.05s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -63.40, time = 24.98s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -62.07, time = 27.07s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -62.50, time = 25.81s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -62.35, time = 27.79s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -61.85, time = 26.18s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -61.53, time = 27.35s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -62.62, time = 24.05s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -61.65, time = 24.38s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -61.46, time = 25.57s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -61.24, time = 27.17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\User\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.99       995\n",
      "         1.0       0.99      0.99      0.99      1121\n",
      "         2.0       0.97      0.98      0.97      1015\n",
      "         3.0       0.98      0.96      0.97      1033\n",
      "         4.0       0.98      0.97      0.97       976\n",
      "         5.0       0.97      0.97      0.97       884\n",
      "         6.0       0.98      0.98      0.98       999\n",
      "         7.0       0.98      0.98      0.98      1034\n",
      "         8.0       0.97      0.96      0.96       923\n",
      "         9.0       0.95      0.97      0.96      1020\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -141.65, time = 18.89s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -118.69, time = 30.54s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -109.50, time = 27.62s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -98.59, time = 27.73s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -93.60, time = 28.32s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -88.63, time = 27.60s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -85.66, time = 27.49s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -83.94, time = 29.10s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -81.22, time = 26.86s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -80.51, time = 24.12s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -78.93, time = 24.20s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -76.82, time = 27.91s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -76.30, time = 25.83s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -73.22, time = 27.67s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -72.94, time = 27.36s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -72.87, time = 26.75s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -71.44, time = 26.60s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -70.79, time = 25.52s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -69.70, time = 26.66s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -69.51, time = 26.36s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -68.39, time = 26.63s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -68.66, time = 26.24s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -67.66, time = 26.23s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -67.44, time = 31.68s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -66.99, time = 29.62s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -67.79, time = 27.44s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -66.19, time = 25.91s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -64.90, time = 28.12s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -63.94, time = 24.32s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -66.29, time = 26.34s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -64.78, time = 25.67s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -64.44, time = 27.93s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -65.76, time = 25.35s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -63.79, time = 25.18s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -63.42, time = 25.05s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -64.47, time = 25.67s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -63.34, time = 26.10s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -63.48, time = 27.46s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -62.39, time = 27.36s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -62.70, time = 27.15s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -62.74, time = 26.97s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -63.33, time = 25.92s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -62.18, time = 26.16s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -61.08, time = 27.16s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -62.59, time = 25.32s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -61.56, time = 26.37s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -62.27, time = 31.16s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -61.72, time = 25.79s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -63.19, time = 25.56s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -61.83, time = 25.89s\n",
      "[BernoulliRBM] Iteration 1, pseudo-likelihood = -272.89, time = 18.12s\n",
      "[BernoulliRBM] Iteration 2, pseudo-likelihood = -250.44, time = 27.46s\n",
      "[BernoulliRBM] Iteration 3, pseudo-likelihood = -233.92, time = 25.04s\n",
      "[BernoulliRBM] Iteration 4, pseudo-likelihood = -222.70, time = 25.62s\n",
      "[BernoulliRBM] Iteration 5, pseudo-likelihood = -217.02, time = 27.70s\n",
      "[BernoulliRBM] Iteration 6, pseudo-likelihood = -214.12, time = 25.84s\n",
      "[BernoulliRBM] Iteration 7, pseudo-likelihood = -212.54, time = 26.16s\n",
      "[BernoulliRBM] Iteration 8, pseudo-likelihood = -207.23, time = 28.22s\n",
      "[BernoulliRBM] Iteration 9, pseudo-likelihood = -207.26, time = 26.16s\n",
      "[BernoulliRBM] Iteration 10, pseudo-likelihood = -203.58, time = 27.04s\n",
      "[BernoulliRBM] Iteration 11, pseudo-likelihood = -202.00, time = 26.28s\n",
      "[BernoulliRBM] Iteration 12, pseudo-likelihood = -204.95, time = 23.98s\n",
      "[BernoulliRBM] Iteration 13, pseudo-likelihood = -202.09, time = 24.23s\n",
      "[BernoulliRBM] Iteration 14, pseudo-likelihood = -196.90, time = 25.20s\n",
      "[BernoulliRBM] Iteration 15, pseudo-likelihood = -195.70, time = 23.69s\n",
      "[BernoulliRBM] Iteration 16, pseudo-likelihood = -197.10, time = 24.80s\n",
      "[BernoulliRBM] Iteration 17, pseudo-likelihood = -197.05, time = 24.25s\n",
      "[BernoulliRBM] Iteration 18, pseudo-likelihood = -195.73, time = 25.09s\n",
      "[BernoulliRBM] Iteration 19, pseudo-likelihood = -194.43, time = 26.18s\n",
      "[BernoulliRBM] Iteration 20, pseudo-likelihood = -193.01, time = 23.95s\n",
      "[BernoulliRBM] Iteration 21, pseudo-likelihood = -190.91, time = 25.33s\n",
      "[BernoulliRBM] Iteration 22, pseudo-likelihood = -193.80, time = 25.95s\n",
      "[BernoulliRBM] Iteration 23, pseudo-likelihood = -193.44, time = 25.44s\n",
      "[BernoulliRBM] Iteration 24, pseudo-likelihood = -191.89, time = 24.84s\n",
      "[BernoulliRBM] Iteration 25, pseudo-likelihood = -191.84, time = 24.57s\n",
      "[BernoulliRBM] Iteration 26, pseudo-likelihood = -191.23, time = 24.67s\n",
      "[BernoulliRBM] Iteration 27, pseudo-likelihood = -191.10, time = 27.22s\n",
      "[BernoulliRBM] Iteration 28, pseudo-likelihood = -189.58, time = 31.86s\n",
      "[BernoulliRBM] Iteration 29, pseudo-likelihood = -190.41, time = 27.52s\n",
      "[BernoulliRBM] Iteration 30, pseudo-likelihood = -189.71, time = 24.44s\n",
      "[BernoulliRBM] Iteration 31, pseudo-likelihood = -189.92, time = 27.22s\n",
      "[BernoulliRBM] Iteration 32, pseudo-likelihood = -191.84, time = 26.21s\n",
      "[BernoulliRBM] Iteration 33, pseudo-likelihood = -188.97, time = 26.52s\n",
      "[BernoulliRBM] Iteration 34, pseudo-likelihood = -188.35, time = 26.34s\n",
      "[BernoulliRBM] Iteration 35, pseudo-likelihood = -188.12, time = 25.44s\n",
      "[BernoulliRBM] Iteration 36, pseudo-likelihood = -187.81, time = 26.70s\n",
      "[BernoulliRBM] Iteration 37, pseudo-likelihood = -189.17, time = 26.73s\n",
      "[BernoulliRBM] Iteration 38, pseudo-likelihood = -189.62, time = 25.85s\n",
      "[BernoulliRBM] Iteration 39, pseudo-likelihood = -187.69, time = 27.18s\n",
      "[BernoulliRBM] Iteration 40, pseudo-likelihood = -185.92, time = 25.71s\n",
      "[BernoulliRBM] Iteration 41, pseudo-likelihood = -188.45, time = 29.17s\n",
      "[BernoulliRBM] Iteration 42, pseudo-likelihood = -186.54, time = 28.86s\n",
      "[BernoulliRBM] Iteration 43, pseudo-likelihood = -188.85, time = 26.40s\n",
      "[BernoulliRBM] Iteration 44, pseudo-likelihood = -187.51, time = 26.06s\n",
      "[BernoulliRBM] Iteration 45, pseudo-likelihood = -188.30, time = 25.39s\n",
      "[BernoulliRBM] Iteration 46, pseudo-likelihood = -186.84, time = 26.66s\n",
      "[BernoulliRBM] Iteration 47, pseudo-likelihood = -187.14, time = 27.23s\n",
      "[BernoulliRBM] Iteration 48, pseudo-likelihood = -188.51, time = 25.89s\n",
      "[BernoulliRBM] Iteration 49, pseudo-likelihood = -186.45, time = 28.13s\n",
      "[BernoulliRBM] Iteration 50, pseudo-likelihood = -185.42, time = 25.71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\User\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       995\n",
      "         1.0       1.00      0.99      0.99      1121\n",
      "         2.0       0.97      0.98      0.98      1015\n",
      "         3.0       0.97      0.96      0.97      1033\n",
      "         4.0       0.98      0.97      0.97       976\n",
      "         5.0       0.96      0.97      0.97       884\n",
      "         6.0       0.99      0.98      0.98       999\n",
      "         7.0       0.98      0.98      0.98      1034\n",
      "         8.0       0.96      0.96      0.96       923\n",
      "         9.0       0.95      0.97      0.96      1020\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Ένα <b>Restricted Boltzmann Machine (RBM)</b> είναι ένα generative στοχαστικό νευρωνικό δίκτυο το οποίο μαθαίνει την κατανομή των δεδομένων που του δίνονται. Χρησιμοποιούνται εν γένει για dimensionality reduction, feature learning, classification και regression μεταξύ άλλων.\n",
    "\n",
    "Ονομάζονται Restricted καθώς αποτελούν παραλλαγή των Boltzmann machines, με τη διαφορά ότι οι <b>δύο τύποι νευρώνων, ορατοί (visible) και κρυφοί (hidden)</b> δεν συνδέονται μεταξύ τους για κάθε ομάδα νευρώνων. Δηλαδή, δεν υπάρχουν συνδέσεις (ακμές μεταξύ κόμβων αν φανταστούμε τους νευρώνες ως κόμβους γράφου) για τους visible κόμβους αναμεταξύ τους όπως και για τους hidden κόμβους αναμεταξύ τους παρά μόνον συνδέσεις μεταξύ visible και hidden κόμβων/νευρώνων. Ένα RBM με 3 νευρώνες visible και 4 νευρώνες hidden φαίνεται παρακάτω στη φωτογραφία.\n",
    "<br>\n",
    "<img src=\"files/Restricted_Boltzmann_machine.png\">\n",
    "<br>\n",
    "\n",
    "Ο περιορισμός αυτός που είναι και η ειδοποιός διαφορά σε σχέση με τα γενικά Boltzmann machines, καθιστά την **εκπαίδευση των RBMs πιο αποδοτική** από αυτή των (γενικών) Boltzmann machines, καθώς αξιοποιούν μεθόδους βασισμένες σε gradient.\n",
    "\n",
    "Αν υποθέσουμε ένα σύνολο **n** το πλήθος **visible νευρώνων** $V = \\{ v_1, v_2, v_3, ..., v_n \\}$ και ένα σύνολο **m** το πλήθος **hidden νευρώνων**  $H = \\{ h_1, h_2, h_3, ..., h_m \\}$, και τα βάρη $w_{ij}$ που ενώνουν τον $i$ visible με τον $j$ hidden νευρώνα. Τότε μπορούμε σε πίνακα να αναπαραστήσουμε τα βάρη των συνδέσεων μεταξύ των visible με των hidden νευρώνων, όπου στις γραμμές έχουμε τους visible και στις στήλες τους hidden νευρώνες.\n",
    "\n",
    "$W = \\begin{bmatrix} w_{11} & w_{12} & w_{13} & ... & w_{1m} \\\\\n",
    "w_{21} & w_{22} & w_{23} & ... & w_{2m}  \\\\\n",
    "w_{31} & w_{32} & w_{33} & ... & w_{3m} \\\\\n",
    "... & ... & ... & ... & ...  \\\\\n",
    "w_{n1} & w_{n2} & w_{n3} & ... & w_{nm}  \\end{bmatrix}$ = $\\begin{bmatrix} 0 & w_{12} & w_{13} & ... & w_{1m} \\\\\n",
    "w_{21} & 0 & w_{23} & ... & w_{2m}  \\\\\n",
    "w_{31} & w_{32} & 0 & ... & w_{3m} \\\\\n",
    " ... & ... & ... & ... & ...  \\\\\n",
    "w_{n1} & w_{n2} & w_{n3} & ... & w_{nm} \\end{bmatrix}$\n",
    "\n",
    ", όπου $w_{ij} = w_{ji}$ για $i \\neq j$, δηλαδή ο πίνακας είναι συμμετρικός και $w_{ii} = 0$. Αυτό μεταφράζεται γραφικά σε ένωση των νευρώνων visible και hidden σα να ήταν κόμβοι ενός γράφου μη κατευθυντικού, και όπου ένας κόμβος δε συνδέεται με τον εαυτό του απευθείας μέσω κάποιας ακμής.\n",
    "\n",
    "Ακόμη, υπάρχουν bias βάρη για τους n visible και m hidden νευρώνες.\n",
    "Biases για visible νευρώνες: $a_1, a_2, a_3, ..., a_n$\n",
    "Biases για hidden νευρώνες: $b_1, b_2, b_3, ..., b_m$\n",
    "\n",
    "Με βάση τα παραπάνω, η **ενέργεια ενός configuration, δηλαδή ενός ζεύγους boolean διανυσμάτων $(\\vec{v},\\vec{h})$**, μπορεί να υπολογιστεί. Π.χ.  $(\\vec{v}=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0\\end{bmatrix},\\vec{h}=\\begin{bmatrix} 1 \\\\ 1 \\\\  0 \\\\ 0\\end{bmatrix})$ για το παράδειγμα της προηγούμενης φωτογραφίας, όπου έχουμε $n=3$ visible nodes και $m=4$ hidden nodes, θα σήμαινε ότι έχουμε μια κατάσταση/configuration όπου είναι ενεργό το 1ο από τα 3 visible units και το 1ο και 2ο από τα hidden units.\n",
    "\n",
    "Η ενέργεια ορίζεται ως:\n",
    "$E(\\vec{v},\\vec{h})=-\\sum_{i=1}^{n}a_iv_i-\\sum_{j=1}^{m}b_jh_j-\\sum_{i=1}^{n}\\sum_{j=1}^{m}v_iw_{ij}h_j$\n",
    "και για το προηγούμενο παράδειγμα θα ήταν:\n",
    "$E(\\vec{v}=\\begin{bmatrix} 1 \\\\ 0 \\\\ 0\\end{bmatrix},\\vec{h}=\\begin{bmatrix} 1 \\\\ 1 \\\\  0 \\\\ 0\\end{bmatrix})=-(α_1v_1+a_2v_2+a_3v_3)-(b_1h_1+b_2h_2+b_3h_3+b_4h_4)-\\sum_{i=1}^{n}\\sum_{j=1}^{m}v_iw_{ij}h_j = -a_1-b_1-b_2-(w_{11}+w_{12})$\n",
    "\n",
    "Ομοίως με τα γενικευμένα Boltzmann machines έπεται η **από κοινού κατανομή πιθανότητας** για τα visible και hidden διανύσματα:\n",
    "$P(\\vec{v},\\vec{h})=\\frac{1}{Z}e^{-E(\\vec{v},\\vec{h})}$\n",
    "με το Z να ισούται με $\\sum_{\\vec{v},\\vec{h}}e^{-E(\\vec{v},\\vec{h})}$ δηλαδή με το άθροισμα των εκθετικών των ενεργειών για όλα τα πιθανά configurations. Αυτό πρακτικά αποτελεί τη σταθερά κανονικοποίησης για την πιθανότητα.\n",
    "\n",
    "Η περιθώρια κατανομή πιθανότητας για ένα visible διάνυσμα θα είναι επομένως το άθροισμα για όλα τα hidden layer configurations της προηγούμενης από κοινού κατανομής πιθανότητας. Δηλαδή:\n",
    "$P(\\vec{v})= \\sum_{\\vec{h}}P(\\vec{v},\\vec{h})=\\sum_{\\vec{h}}\\frac{1}{Z}e^{-E(\\vec{v},\\vec{h})}=\\frac{1}{Z}\\sum_{\\vec{h}}e^{-E(\\vec{v},\\vec{h})}$ και ομοίως μιας και είναι bipartite ο γράφος τα hidden units είναι ανεξάρτητα δεδομένου των visible unit activations και προκύπτει για την περιθώρια κατανομή πιθανότητας ενός hidden διανύσματος:\n",
    "$P(\\vec{h})= \\sum_{\\vec{v}}P(\\vec{v},\\vec{h})=\\sum_{\\vec{v}}\\frac{1}{Z}e^{-E(\\vec{v},\\vec{h})}=\\frac{1}{Z}\\sum_{\\vec{v}}e^{-E(\\vec{v},\\vec{h})}$. Με τον ίδιο τρόπο, τα visible units είναι ανεξάρτητα δεδομένου των hidden unit activations και άρα έχουμε:\n",
    "$P(\\vec{v}|\\vec{h})=\\prod_{i=1}^{n}P(v_i|\\vec{h})$ και\n",
    " $P(\\vec{h}|\\vec{v})=\\prod_{j=1}^{m}P(h_j|\\vec{v})$\n",
    "\n",
    "Η περίπτωση να έχουμε activation ενός μόνο νευρώνα i visible ($\\vec{v}=[0, 0, ..., 0, v_i, 0, ..., 0]^T$) δεδομένου ενός configuration $\\vec{h}$ για το hidden state έχει πιθανότητα:\n",
    "$P(v_i=1|\\vec{h})=P(\\vec{v}=[0,0,...,0,v_i​,0,...,0]^T|\\vec{h})=\\frac{P(\\vec{v}=[0,0,...,0,v_i​,0,...,0, \\vec{h})}{P(\\vec{h})}=\\frac{\\frac{1}{Z}e^{-E(\\vec{v}=[0,0,...,0,v_i​,0,...,0]^T,\\vec{h})}}{\\frac{1}{Z}\\sum_{\\vec{v}}e^{-E(\\vec{v},\\vec{h})}}=\\frac{e^{-E(\\vec{v}=[0,0,...,0,v_i​,0,...,0]^T,\\vec{h})}}{\\sum_{\\vec{v}}e^{-E(\\vec{v},\\vec{h})}}$.\n",
    "\n",
    "Στην περίπτωση αυτή που το $\\vec{v}$ έχει μόνο μία μη μηδενική τιμή στη θέση $i$, προκύπτει η ενέργεια:\n",
    "$E(\\vec{v}=[0,0,...,0,v_i​,0,...,0]^T,\\vec{h})=-a_i-\\sum_{j=1}^{m}w_{ij}h_j$ και άρα:\n",
    "$P(v_i=1|\\vec{h})=σ(a_i + \\sum_{j=1}^{m}w_{ij}h_j)$, όπου $σ$ είναι η σιγμοειδής συνάρτηση $σ(x)=\\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "Ο αλγόριθμος εκπαίδευσης για τα RBM αφορά τη μεγιστοποίηση του γινομένου πιθανοφάνειας ενός training set μεγέθους k, $V = \\{\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_k}\\}$:\n",
    "$arg max_W \\prod_{\\vec{u} \\in V}P(\\vec{v})$\n",
    "Δηλαδή δίνεται ένα δείγμα εκπαίδευσης με k δειγματικά στοιχεία (διανύσματα). Με βάση τα όσα αναφέραμε προηγουμένως καθένα από αυτά τα k διανύσματα εκπαίδευσης $\\vec{v_1}, \\vec{v_2}, ..., \\vec{v_k}$ έχει διάσταση n (δηλαδή έχουμε n χαρακτηριστικά/features στο dataset), και το RBM κωδικοποιεί αυτές τις n διαστάσεις/χαρακτηριστικά σε m (μέσω των hidden states), που είναι και μια εφαρμογή των RBMs για dimensionality reduction όταν m (hidden states) < n (visible states).\n",
    "\n",
    "Ο πιο συνήθης αλγόριθμος εκπαίδευσης για τα RBMs, δηλαδή για την εύρεση βέλτιστης τιμής για τον πίνακα βαρών **W**, είναι ο contrastive divergence algorithm που πραγματοποιεί Gibbs sampling και χρησιμοποιείται εντός μιας gradient descent διαδικασίας για την ανανέωση των βαρών. Ο contrastive divergence (CD) algorithm αποτελεί μια προσέγγιση της Maximum Likelihood Estimation (MLE) μεθόδου που θα εφαρμοζόταν ιδανικά για την εκμάθηση των βαρών. Τελικά τα βάρη ανανεώνονται σε ένα RBM ως εξής:\n",
    "\n",
    "$w_{ij}(t_1)=w_{ij}(t)+η \\frac{\\partial log(P(\\vec{v}))}{\\partial w_{ij}}$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<li>Ποια είναι η λογική των <i>DBN</i> και σε τι προβλήματα τα αξιοποιούμε;</li>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Τα Deep Belief Networks (DBNs) είναι generative μοντέλα που αποτελούνται από συνδυασμό πολλαπλών Restricted Boltzmann Machines (RBMs), όπου για κάθε RBM layer που χρησιμοποιείται, το hidden layer του προηγούμενου RBM είναι το visible layer του επόμενου.\n",
    "\n",
    "Τα DBN εκαπιδεύεται one layer at a time, δηλαδή τα RBMs εκπαιδεύονται το ένα μετά το άλλο ξεκινώντας από το 1o visible layer (layer εισόδου).\n",
    "\n",
    "Τα διάφορα layers λειτουργούν σαν feature detectors, δηλαδή εξάγουν χαρακτηριστικά από τα δεδομένα μας. Επίσης, ένα DBN μπορεί να εκαπιδευτεί επιβλεπόμενα για classification. Τα DBNs έχουν γενικώς χρησιμοποιηθεί για generation και classification εικόνων, video sequences και motion-capture data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<li>Να αναφέρετε τις βασικότερες εφαρμογές των <i>RBM</i> και <i>DBN</i>.</li>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Όπως αναφέρθηκε προηγουμένως, τα RBMs είναι δίκτυα που μαθαίνουν την κατανομή των δεδομένων που τους δίνονται. Χρησιμοποιούνται εν γένει για dimensionality reduction, feature learning, classification και regression μεταξύ άλλων και έχουν εφαρμογή σε εικόνες λόγου χάρη.\n",
    "\n",
    "Ομοίως, τα DBNs που είναι συνδυασμός layers από RBMs, χρησιμοποιούνται για να παράγουν και να κάνουν classify εικόνες, video sequences και motion-capture data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<li>Εκτός από <i>RBM</i>, τι άλλα μοντέλα μπορούν να χρησιμοποιηθούν για να δημιουργήσουν <i>DBN</i>.</li>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Μια άλλη επιλογή αντί για layers από RBMs για τη δημιουργία DBNs είναι οι autoencoders."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<li>Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo <i>Logistic Regression</i> χωρίς τη χρήση <i>RBM</i> σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η <i>RBM</i> καθώς και με αυτή όπου χρησιμοποιούνται <i>RBM</i> και <i>DBN</i> για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;</li>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Με βάση τα αποτελέσματα της ταξινόμησης, βλέπουμε πως η ταξινόμηση με RBM και DBN βελτιώνει σημαντικά την ταξινόμηση. Ενώ ίσως θα περιμέναμε μια κάποια βελτίωση χρησιμοποιώντας DBN αντί RBM, δεν είδαμε κάτι τέτοιο τελικά. Βέβαια, οι μετρικές ήταν ήδη πολύ υψηλά για το RBM. Βλέπουμε λοιπόν πως η αναπαράσταση των αρχικών χαρακτηριστικών με νέα μέσω ενός (απλό RBM) ή δύο (DBN) RBM layers επιφέρει σημαντική βελτίωση στις μετρικές (πχ accuracy)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "References:\n",
    "- [Medium - What Are RBMs, Deep Belief Networks and Why Are They Important to Deep Learning?](https://medium.com/swlh/what-are-rbms-deep-belief-networks-and-why-are-they-important-to-deep-learning-491c7de8937a)\n",
    "- [YouTube - Restricted Boltzmann Machines (RBM) - A friendly introduction](https://www.youtube.com/watch?v=Fkw0_aAtwIw)\n",
    "- [Wikipedia - Restricted Boltzmann machine](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)\n",
    "- [Wikipedia - Boltzmann machine](https://en.wikipedia.org/wiki/Boltzmann_machine)\n",
    "- [Wikipedia - Deep belief network](https://en.wikipedia.org/wiki/Deep_belief_network)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}