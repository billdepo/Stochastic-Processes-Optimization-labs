{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHFOjD_VY0ld"
   },
   "source": [
    "<h1>Q-Learning</h1>\n",
    "\n",
    "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείγματος θα εξετάσετε μία υλοποίηση με Q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
    "<ul>\n",
    "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
    "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
    "<li>Να τηρούνται οι κανόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
    "</ul>\n",
    "\n",
    "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p> \n",
    "\n",
    "<h4>Επιβράβευση</h4>\n",
    "<ul>\n",
    "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
    "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
    "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
    "</ul>\n",
    "\n",
    "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
    "\n",
    "<h4>Πλήθος Καταστάσεων</h4>\n",
    "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
    "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
    "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
    "\n",
    "<h4>Πλήθος Ενεργειών</h4>\n",
    "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
    "<ul>\n",
    "<li>0=Νότια</li>\n",
    "<li>1=Βόρεια</li>\n",
    "<li>2=Ανατολικά</li>\n",
    "<li>3=Δυτικά</li>\n",
    "<li>4=Επιβίβαση</li>\n",
    "<li>5=Αποβίβαση</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOQpQ0DygDzt"
   },
   "source": [
    "<p>Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n",
    "\n",
    "<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "nEPfGXmtgyPA"
   },
   "source": [
    "Ο αλγόριθμος Q-Learning ανήκει στην κατηγορία του Reinforcement Learning και πιο συγκεκριμένα εντάσσεται στην υποκατηγορία του **model-free learning**, όπου ο agent **δεν** προσπαθεί να μάθει ρητά τις environment state transition και reward functions, αλλά καταλήγει σε ένα βέλτιστο policy/πολιτική από τις αλληλεπιδράσεις του με το περιβάλλον. Ουσιαστικά, ο τρόπος που το κάνει αυτό είναι μαθαίνοντας καλά και κακά actions μέσω trial and error.\n",
    "\n",
    "Ο Q-Learning αλγόριθμος προσεγγίζει τη συνάρτηση Q-function από τα δείγματα του $Q(s,a)$ που παρατηρούνται κατά την αλληλεπίδραση του agent με το environment.\n",
    "\n",
    "Ο αλγόριθμος έχει ως εξής σε ψευδοκώδικα:\n",
    "- Ο πίνακας $Q(s,a)$ αρχικοποιείται τυχαία\n",
    "- Για κάθε interaction του agent με το environment:\n",
    "    - παρατηρείται το reward του action του $r(s,a)$ και το state transition (νέο state $s'$)\n",
    "    - υπολογίζεται το observed Q-value $Q_{o b s}(s, a)=r(s, a)+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime}\\right)$\n",
    "    - ανανέωση της τιμής του Q-function $Q(s,a)$ ως εξής $Q(s, a)=(1-\\eta) Q(s, a)+\\eta \\cdot Q_{o b s}(s, a)$\n",
    "\n",
    "όπου $0< \\eta \\leq 1$ είναι το learning rate (ρυθμός μάθησης) και $0 \\leq \\gamma \\leq 1$ είναι το discount factor όπως είχε περιγραφεί και στο εργαστήριο με τους Value και Policy Iteration αλγορίθμους. Αυτός ο παράγοντας δίνει μικρότερα βάρη σε μελλοντικά rewards. Θέτοντας τιμή 0 στο $\\gamma$ ουσιαστικά κάνουμε τον agent να ενδιαφέρεται μόνο για άμεσα (short-term) rewards, δηλαδή να είναι greedy, ενώ αν θέσουμε τιμή ίση με 1 τότε ο agent δίνει ίδιο βάρος σε short-term και long-term rewards.\n",
    "\n",
    "Το Reinforcement Learning βρίσκεται ενδιάμεσα στο Supervised και Unsupervised Learning. Ο στόχος στο Reinforcement Learning είναι η εύρεση της βέλτιστης ακολουθίας αποφάσεων που επιτρέπουν σε έναν agent να λύσει ένα πρόβλημα μεγιστοποίησης μακροπρόθεσμου reward. Αυτή η ακολουθία actions μαθαίνεται μέσω αλληλεπίδρασης του agent με το environment και την παρατήρηση των rewards σε κάθε state.\n",
    "Για παράδειγμα, παιχνίδια όπως το Go, είναι κλασικά παραδείγματα εφαρμογής Reinforcement Learning.\n",
    "\n",
    "Η βασική διαφορά του Q-learning από το Value and Policy iteration αλγορίθμους έγκειται στο γεγονός ότι ο Value and Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pc11bCDZgfSx"
   },
   "source": [
    "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "y_95lIAzWYpp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8a1ca24c-bc2b-45a3-f48d-ec1a986a3ce6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.22.4)\n",
      "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
      "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NDHLb5PGWdwr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "97b2cc39-731c-4ba8-a803-c2ed0a514ae9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001B[35mY\u001B[0m|\u001B[43m \u001B[0m: |\u001B[34;1mB\u001B[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "396kTtOrW-cO",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "541bae3f-5496-469b-c136-53466c53eba7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------+\n",
      "|\u001B[34;1mR\u001B[0m: | : :\u001B[35mG\u001B[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001B[43m \u001B[0m|\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UC6-XuIhF5_"
   },
   "source": [
    "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nPSOw5CdXFx1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c839f7b7-3180-4ad1-ba5e-8d3d2419fc65"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "State: 328\n",
      "+---------+\n",
      "|\u001B[35mR\u001B[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001B[43m \u001B[0m: | : |\n",
      "|\u001B[34;1mY\u001B[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsAkQJhchVFy"
   },
   "source": [
    "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "j7oDbznIXOJo",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "33986771-3af8-42e4-ab78-60e1d437a1a6"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "env.P[328]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Δημιουργώντας το environment, δημιουργείται και ένα dictionary $P$ με keys τα states (κωδικοποίηση 0 έως 499 για τα 500 πιθανά states συνολικά) και values ένα dictionary για κάθε state, το οποίο με τη σειρά του έχει ως keys τα δυνατά actions (κωδικοποίηση 0 έως 5 για τα 6 πιθανά actions συνολικά) και ως values μια λίστα με μία tuple μέσα. Αρχικά οι τιμές που δημιουργούνται είναι κάποιες αυθαίρετες default values.\n",
    "\n",
    "Άρα για ένα επιλεγμένο state ως key στο dictionary παίρνουμε ως value το εξής:\n",
    "\n",
    "{action0: [(probability0, nextstate0, reward0, done0)],<br>\n",
    "action1: [(probability1, nextstate1, reward1, done1)],<br>\n",
    "action2: [(probability2, nextstate2, reward2, done2)],<br>\n",
    "action3: [(probability3, nextstate3, reward3, done3)],<br>\n",
    "action4: [(probability4, nextstate4, reward4, done4)],<br>\n",
    "action5: [(probability5, nextstate5, reward5, done5)]}\n",
    "\n",
    "Τα στοιχεία της κάθε tuple με τη σειρά είναι:\n",
    "- Η πιθανότητα να συμβεί το intended action του agent\n",
    "- Το next state id (0-499)\n",
    "- Tο reward του agent για το action αυτό\n",
    "- Ένα boolean flag αν είναι done ή όχι μια διαδρομή, δηλαδή αν επιτυχώς ο επιβάτης του ταξί έφτασε στον προορισμό του.\n",
    "\n",
    "Το mapping των actions έχει ως εξής:\n",
    "- 0: south\n",
    "- 1: north\n",
    "- 2: east\n",
    "- 3: west\n",
    "- 4: pickup\n",
    "- 5: dropoff\n",
    "\n",
    "ενώ η πιθανότητα είναι πάντα 1, δηλαδή ο τρόπος με τον οποίο συμβαίνουν οι κινήσεις του agent είναι deterministic. Αυτό σημαίνει ότι αν ένας agent αποφασίσει να κάνει ένα action, πχ να κινηθεί προς τα πάνω, τότε με $100\\%$ πιθανότητα θα κινηθεί προς τα πάνω. Ακόμη, όλες οι κινήσεις έχουν ως reward $-1$, ώστε να προσπαθήσει ο agent να ελαχιστοποιήσει τον αριθμό κινήσεών του προς την πλήρωση του σκοπού του, ενώ οι pickup/dropoff actions έχουν reward $-10$ αν δεν είναι οι σωστές τοποθεσίες για pickup/dropoff του πελάτη του ταξί, ενώ αν είναι σωστό το action του dropoff (σωστό dropoff location/state), τότε το reward είναι $+20$. Κάθε επιτυχημένο dropoff ορίζει το τέλος ενός episode.\n",
    "\n",
    "Τέλος, να σημειώσουμε πως η κίνηση προς κάποιον τοίχο συνεπάγεται παραμονή στο ίδιο κελί/state του ταξί και reward -1, πράγμα που οδηγεί τον agent στο να κινείται και να μην παραμένει σε ένα κελί.\n",
    "\n"
   ],
   "metadata": {
    "id": "0evBjsFAsloH"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtROeD1ph6kk"
   },
   "source": [
    "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
    "\n",
    "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "vKHhcDxVXUFz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e5a134b4-c0b9-41c5-ac71-9f9a35802d7e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Timesteps taken: 3038\n",
      "Penalties incurred: 987\n"
     ]
    }
   ],
   "source": [
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Το παραπάνω cell κώδικα, επιλέγει τυχαία όταν βρίσκεται σε κάθε state, ένα action από τα 6 πιθανά actions του agent, τα οποία είναι τα εξής:\n",
    "- 0 = south\n",
    "- 1 = north\n",
    "- 2 = east\n",
    "- 3 = west\n",
    "- 4 = pickup\n",
    "- 5 = dropoff\n",
    "\n",
    "Με βάση το τυχαία επιλεγμένο action οδηγείται σε ένα νέο state ενώ λαμβάνει και ένα reward. \n",
    "\n",
    "Παρατηρούμε ότι ο agent χρειάζεται χιλιάδες βηματα για να καταφέρει να παραδώσει κάποιον πελάτη του ταξί στην σωστή τοποθεσία, ενώ στο μεταξύ κάνει σχεδόν 1000 penalties (!), δηλαδή κάνει ισάριθμες φορές λάθος στο pickup/dropoff του πελάτη.\n",
    "\n",
    "Αυτό συμβαίνει γιατί ο agent δε μαθαίνει και λειτουργεί με τυχαίο τρόπο. Αντιθέτως, με τη χρήση του Q-Learning, ο agent αποκτά μνήμη και μπορεί να μάθει με τρόπο trial and error από τις προηγούμενες προσπάθειές του μαθαίνοντας τελικά την καλύτερη action για κάθε state ώστε να μεγιστοποιεί το reward του."
   ],
   "metadata": {
    "id": "4XwnIgSxqo7B"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj3s09rsizVm"
   },
   "source": [
    "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
    "\n",
    "<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Η παράμετρος **α** είναι το learning rate (παραπάνω δόθηκε ως **η** στις εξισώσεις του Q-function), δηλαδή ο ρυθμός με τον οποίο ανανεώνεται το Q-value σε κάθε βήμα. Τιμή 1 σημαίνει ότι ανανεώνεται η τιμή του Q για κάθε state και action χωρίς να υπολογίζεται η προηγούμενη τιμή του Q-table (πίνακας με γραμμές τα states και columns τα actions). Γενικώς, το learning rate είναι μια παράμετρος που δείχνει πόση γνώση συγκρατεί από την προηγούμενη εμπειρία του ο agent σε κάθε βήμα.\n",
    "\n",
    "Η παράμετρο **γ** μοντελοποιεί την ανάθεση βαρών σε μελλοντικά rewards. Θέτοντας τιμή 0 στο **γ** ουσιαστικά κάνουμε τον agent να ενδιαφέρεται μόνο για άμεσα (short-term) rewards, δηλαδή να είναι greedy, ενώ αν θέσουμε τιμή ίση με 1 τότε ο agent δίνει ίδιο βάρος σε short-term και long-term rewards."
   ],
   "metadata": {
    "id": "9OPKfSjR5nv1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JJk3NTfcXrrA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Oy2Yg8DTXtHW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3ed9f26c-8dec-475f-b4f8-a07a3bde7e70"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 5s, sys: 10.8 s, total: 1min 16s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dxt4fmvGYBOm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d67f23cb-e778-42a9-965a-5a2cea3cebb2"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ -2.40991108,  -2.27325184,  -2.40616307,  -2.36013082,\n",
       "       -10.60593154, -10.6527113 ])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "q_table[328]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6W9JE9yOYGgP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a39dc108-b67a-4a9f-c549-2eacd8bf7111"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.91\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R7gW1nLj-qE"
   },
   "source": [
    "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
    "<ul>\n",
    "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
    "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
    "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
    "</ul>\n",
    "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Σε 100 επεισόδια:\n",
    "Average timesteps per episode: 12.91\n",
    "Average penalties per episode: 0.0\n",
    "\n",
    "| Measure |Random agent's performace values| Q-learning agent's performance   values|   \n",
    "|---|---|---|\n",
    "| Μέσος αριθμός παραβάσεων ανά επεισόδιο  | 987 | 0.0 |  \n",
    "| Μέσος αριθμός βημάτων ανά διαδρομή  | 3038 | 12.91  |   \n",
    "| Μέσος αριθμός ανταμοιβών ανά κίνηση  | $\\frac{987 \\cdot (-10) + (3038 - 987 - 1) \\cdot (-1) + 20}{3038} = -3.917$  |  $\\frac{0 \\cdot (-10) + (12.91 - 0 - 1) \\cdot (-1) + 20}{12.91} = \\frac{-11.91+20}{12.91} = 0.627$  |   \n",
    "\n",
    "Είναι εμφανές, ότι ο Q learning agent είναι σαφώς καλύτερος από τον random καθώς χρειάζεται λιγότερα από 13 βήματα κατά μέσο όρο για να μεταφέρει καθέναν από τους 100 πελάτες του ταξί και δεν κάνει λανθασμένα actions που καταλήγουν σε reward $-10$."
   ],
   "metadata": {
    "id": "JxpiGF_E_p7X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Information sources:\n",
    "- <a href=\"https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\">Q-Learning - OpenAI gym taxi</a>\n",
    "- <a href=\"https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\">Value and policy iteration for the Frozen Lake problem</a>"
   ],
   "metadata": {
    "id": "yZ1narE3nAZN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    ""
   ],
   "metadata": {
    "id": "6ySP8feZnMfm"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Depastas_lab7_Q_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}