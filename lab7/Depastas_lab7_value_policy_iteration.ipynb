{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_7_Markov_Decision_Processes).ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLZdEbAy2jfr",
    "colab_type": "text"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VsUV229__zO",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OM8ivgOJAg_H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_puV3ugeAnbU",
    "colab_type": "text"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ep-MvIUCAxT8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ],
   "execution_count": 77,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-22 19:43:38,036] Making new env: FrozenLake8x8-v0\n",
      "C:\\Users\\User\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBKBXJDUBRUh",
    "colab_type": "text"
   },
   "source": [
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p6lqbG9zBgdi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "env.reset()\n",
    "env.render()"
   ],
   "execution_count": 78,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[41mS\u001B[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ipykernel.iostream.OutStream at 0x1e074e6e740>"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX2res4JBlYb",
    "colab_type": "text"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gq7q944YBx0Q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "next_action = env.action_space.sample()\n",
    "env.step(next_action)\n",
    "env.render()"
   ],
   "execution_count": 113,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFF\u001B[41mH\u001B[0mFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": "<ipykernel.iostream.OutStream at 0x1e074e6e740>"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV4A7lsLB54y",
    "colab_type": "text"
   },
   "source": [
    "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Τρέχοντας κάθε φορά τις τελευταίες εντολές, ο agent κάνει μία κίνηση η οποία δεν είναι ντετερμινιστική, αλλά στοχαστική. Δηλαδή παρότι θέλει να κάνει ένα action για να κινηθεί προς μια κατεύθυνση όντας σε ένα state του grid της λίμνης, τότε με μία πιθανότητα θα κινηθεί προς τα κει αλλά αυτή δε θα είναι 1. Αυτό το βλέπουμε τρέχοντας τον κώδικα, όπου το action δίνεται σε παρένθεση και βλέπουμε που τελικά θα βρεθεί ο agent.\n",
    "\n",
    "Μάλιστα για το λόγο αυτό, βλέπουμε ότι ο agent εγκλωβίζεται σε ένα hole/τρύπα (H) τελικά της λίμνης, από όπου έπειτα δεν μπορεί να ξεφύγει."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAL4we3gDV_J",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQKm4VAUChi1",
    "colab_type": "text"
   },
   "source": [
    "<ul>\n",
    "<li>Μελετώντας <a href=\"https://machinelearningjourney.com/index.php/2020/07/02/frozenlake/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 1\n",
    "<li>Μελετώντας <a href=\"https://machinelearningjourney.com/index.php/2020/07/02/frozenlake/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Στο πρόβλημα της παγωμένης λίμνης υπάρχει ένα grid 64 θέσεων (8x8) εντός του οποίου μπορεί να κινηθεί ο agent. Ο agent εκκινεί από τη θέση/state 0 (1η θέση του grid πάνω αριστερά, denoted με S) και αναλόγως το action που επιλέγεται σε κάθε state (4 πιθανά actions: 0 -> left, 1 -> down, 2 -> right, 3 -> up), θα κινηθεί ο agent με πιθανότητα 1/3 σε κάθε κατεύθυνση πέραν της αντίθετης από το action. Επί παραδείγματι, αν είναι σε ένα state s ο agent, και επιλέξει ως action να κινηθεί προς τα πάνω, τότε με πιθανότητα 1/3 θα κινηθεί είτε πάνω, αριστερά ή δεξιά (η μόνη κίνηση που έχει μηδενική πιθανότητα να συμβεί είναι προς τα κάτω, δηλαδη αντίθετα από το επιλεγμένο action).\n",
    "\n",
    "O agent έχει ως στόχο εκκινώντας από το starting state S (state 0) να φτάσει στο terminal state G (state 63, κάτω δεξιά κυτίο του 8x8 grid) κινούμενος στα ενδιάμεσα states F όπου είναι παγωμένη η λίμνη, και να αποφύγει κάποιες τρύπες/holes (denoted με H), στις οποίες αν βρεθεί δεν έχει τη δυνατότητα να φύγει (παραμένει με πιθανότητα 1). Επίσης, αν φτάσει στο terminal state G ο agent, τότε παραμένει με πιθανότητα 1, ενώ αν ένα action βγάζει τον agent εκτός χάρτη τότε αυτό συνεπάγεται παραμονή του agent στο ίδιο state.\n",
    "\n",
    "Είναι εμφανές με βάση την παραπάνω περιγραφή, ότι το περιβάλλον του agent είναι στοχαστικό και όχι ντετερμινιστικό. Αυτό οφείλεται στη φύση του προβλήματος, αφού μιλάμε για παγωμένη λίμνη η οποία γλιστράει και επομένως αν ο agent θέλει να κινηθεί προς μια κατεύθυνση (πχ action up), τότε δεν είναι σίγουρο ότι θα κινηθεί εκεί τελικώς (μπορεί με ίση πιθανότητα να κινηθεί up, left ή right).\n",
    "\n",
    "Αυτό που επιθυμούμε να πετύχουμε, είναι να μάθει ο agent το policy εκείνο των κινήσεων για κάθε state που να μεγιστοποιεί τη value function. Πρακτικά αυτό σημαίνει να μάθει ο agent ποιο είναι το βέλτιστο εκείνο action που δυνητικά μπορεί να τον οδηγήσει στο terminal state G."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 2\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Σύμφωνα με τη <a href=\"https://en.wikipedia.org/wiki/Markov_property\"></a> ο ορισμός της ιδιότητας Markov είναι ο εξής:\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F}, P)$ be a probability space with a filtration $\\left(\\mathcal{F}_{s}, s \\in I\\right)$, for some (totally ordered) index set $I$; and let $(S, \\mathcal{S})$ be a measurable space. A $(S, \\mathcal{S})$-valued stochastic process $X=\\left\\{X_{t}: \\Omega \\rightarrow S\\right\\}_{t \\in I}$ adapted to the filtration is said to possess the Markov property if, for each $A \\in \\mathcal{S}$ and each $s, t \\in I$ with $s<t$,\n",
    "$$\n",
    "P\\left(X_{t} \\in A \\mid \\mathcal{F}_{s}\\right)=P\\left(X_{t} \\in A \\mid X_{s}\\right) .{ }^{[3]}\n",
    "$$\n",
    "In the case where $S$ is a discrete set with the discrete sigma algebra and $I=\\mathbb{N}$, this can be reformulated as follows:\n",
    "$$\n",
    "P\\left(X_{n}=x_{n} \\mid X_{n-1}=x_{n-1}, \\ldots, X_{0}=x_{0}\\right)=P\\left(X_{n}=x_{n} \\mid X_{n-1}=x_{n-1}\\right) .\n",
    "$$\n",
    "\n",
    "Ο παραπάνω ορισμός δείχνει ότι η πιθανότητα να βρεθεί ο agent σε μια κατάσταση εξαρτάται μόνο από την προηγούμενη κατάσταση.\n",
    "Στο συγκεκριμένο πρόβλημα του Frozen Lake 8x8, έχουμε 64 θέσεις στις οποίες μπορεί να βρίσκεται ο agent. Κάθε επόμενη κίνησή του, λοιπόν, από μια κατάσταση s εξαρτάται μόνο από την τρέχουσα αυτή κατάσταση s. Αυτό διευκολύνει τη μελέτη του προβλήματος αφού δε μας ενδιαφέρουν όλες οι προηγούμενες καταστάσεις στις οποίες μπορεί να βρισκόταν."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 3\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Οι δύο αλγόριθμοι, value iteration και policy iteration είναι αλγόριθμοι εύρεσης της βέλτιστης πολιτικής (policy) κινήσεων (actions) ενός agent όταν αυτός γνωρίζει επαρκώς λεπτομέρειες (sufficient details) του μοντέλου του περιβάλλοντος (environment) σε ένα Markov Decision Process πρόβλημα.\n",
    "\n",
    "Ο agent αλληλεπιδρά με το environment και σε κάθε time step κάνει perform ένα action που καταλήγει σε:\n",
    "- αλλαγή του state του environment\n",
    "- επιβράβευση (reward) ή ποινή (penalty) από το environment\n",
    "\n",
    "Ο στόχος του agent είναι να ανακαλύψει το βέλτιστο policy, δηλαδή τι actions να κάνει perform σε κάθε state, ώστε να μεγιστοποιήσει τη συνολική τιμή των rewards που λαμβάνει από το environment ως αποτέλεσμα των actions του.\n",
    "\n",
    "Το policy είναι ουσιαστικά μια συνάρτηση $\\pi(s)$ με την οποία αποφασίζει ο agent τι action θα κάνει perform: $\\pi(s): S -> A$, όπου $S$: το σύνολο των states (του environment) και $A$ το σύνολο των actions που κάνει perform ο agent.\n",
    "\n",
    "Το environment μπορεί να ναι ντετερμινιστικό ή στοχαστικό. Όταν είναι ντετερμινιστικό, δεν υπάρχει αβεβαιότητα για έναν agent που κάνει plan τα actions του και το environment μπορεί να μοντελοποιηθεί σαν ένας γράφος με κάθε κόμβο να παριστάνει ένα state και κάθε βάρος ακμής να εκφράζει το reward.\n",
    "\n",
    "Ως παράδειγμα, δίνουμε το παρακάτω. Έστω ένας agent που αρχικά είναι στο state $s_0$. Τότε:\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{tabular}{||c c c c c c||}\n",
    " \\hline\n",
    " time\\ 0 & time\\ 1 & time\\ 2 & ... & time\\ i & ... \\\\\n",
    " \\hline\\hline\n",
    " agent\\ observes\\ environment: $s_0$ & $s_1$ & $s_2$ & ... & $s_i$ & ...\\\\\n",
    " \\hline\n",
    "picks\\ action: $a_0$ & $a_1$ & $a_2$ & ... & $a_i$ & ...\\\\\n",
    " \\hline\n",
    " upon\\ performing\\ action,\\ state: $s_1$  &  $s_2$ &  $s_3$ & ... &   $s_{i+1}$  & ...\\\\\n",
    " \\hline\n",
    "reward: $r_1$ & $r_2$ & $r_3$ & ... & $r_{i+1}$ & ...\\\\\n",
    " \\hline\n",
    "\\end{tabular}\n",
    "\\end{center}\n",
    "\n",
    "Οπότε για το παράδειγμα αυτό το συνολικό reward που λαμβάνει ο agent είναι $TR = r_1 + r_2 + r_3 + ... + r_i + ... = \\sum_{i=1}^{T}r_i$, όπου το $Τ$ που ονομάζεται και horizon ή episodes length μπορεί να απειρίζεται.\n",
    "\n",
    "Συνήθως στα rewards που λαμβάνει ένας agent υπεισέρχεται και ένα discount factor το οποίο εμποδίζει το συνολικό reward TR να απειρίζεται και δίνει μεγαλύτερο βάρος στα βραχυπρόθεσμα (πιο πρόσφατα) rewards έναντι των μακροπρόθεσμων. Στην περίπτωση αυτή έχουμε $TR = r_1 + \\gamma \\cdot r_2 + \\gamma^2 \\cdot r_3 + ... + \\gamma^{i-1} \\cdot r_i + ... = \\sum_{i=1}^{T}\\gamma^{i-1} \\cdot r_i$. Αυτή η περίπτωση είναι πιο γενική αφού για $\\gamma = 1$ δίνετι την προηγούμενη.\n",
    "\n",
    "Στη συνέχεια ορίζουμε τη value function $V(s)$ που αντιπροσωπεύει πόσο καλό είναι ένα state για τον agent να είναι σε αυτό ή ισοδύναμα δίνει το συνολικό αναμενόμενο reward για τον agent αν ξεκινούσε από το state s και εξαρτάται από το policy $\\pi (s)$ με το οποίο επιλέγει actions ο agent.\n",
    "\n",
    "$ V^{\\pi}(s) = E[TR] = \\sum_{i=1}^{T}\\gamma^{i-1} \\cdot r_i $, για κάθε $s \\in S$\n",
    "Από όλες τις πιθανές value functions $ V^{\\pi}(s)$, υπάρχει μια βέλτιστη $ V^{*}(s)$ με υψηλότερη τιμή από τις άλλες για κάθε state s:\n",
    "$ V^{*}(s) = \\max_{\\pi}V^{\\pi (s)}$, για κάθε $s \\in S$.\n",
    "Τότε η βέλτιστη policy είναι αυτή που αντιστοιχεί στη βέλτιση value function:\n",
    "$\\pi ^{*} = arg \\max_{\\pi}\\{V^{\\pi}(s)\\}$, για κάθε $s \\in S$.\n",
    "\n",
    "Ακόμη ορίζεται η συνάρτηση state-action pair $Q: S x A -> R$, όπου η βλετιστη Q-function $Q^*(s,a)$ είναι το αναμενόμενο συνολικό reward για έναν agent που εκκινή από το state s και διαλέγει να κάνει perform το action a. Οπότε το optimal value function $ V^{*}(s)$ είναι το μέγιστο $Q^*(s,a)$ σε όλα τα πιθανά actios, δηλαδή $V^{*}(s) = \\max_{a}Q^*(s,a)$ , για κάθε $s \\in S$. Αν γνωρίζουμε την $Q^*(s,a)$ τότε μπορεί να βρεθεί το βέλτιστο policy $\\pi ^{*}$ επιλέγοντας εκείνο το action a ου δίνει το μέγιστο $Q^*(s,a)$ για το state s. Άρα: $\\pi ^{*} = arg \\max_{a} Q^*(s,a)$, για κάθε $s \\in S$.\n",
    "\n",
    "Έπειτα, το Bellman equation, παρέχει έναν αναδρομικό ορισμό της $Q^*(s,a)$ ως εξής:\n",
    "$Q^*(s,a) = R(s,a) + \\gamma E_{s'}[V^{*}(s')]= R(s,a) + \\gamma \\sum_{s' \\in S} p(s'|s,a) \\cdot V^{*}(s') \\rightarrow V^{*}(s) = \\max_{a}Q^*(s,a) = \\max_{a} [R(s,a) + \\gamma \\sum_{s' \\in S} p(s'|s,a) \\cdot V^{*}(s')]$.\n",
    "\n",
    "Οι αλγόριθμοι value iteration και policy iteration βασίζονται στις παραπάνω εξισώσεις για τον υπολογισμό της βέλτιστης value function $V^*(s)$ και κατ' επέκταση του αντίστοιχου βέλτιστου policy $\\pi^*$ δεδομένου των συνόλων των states $S$ και των ctions $A$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Value iteration algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Value iteration**\n",
    "- Υπολογίζει:βέλτιστο state value function $V^*(s)$\n",
    "- Πώς: επαναληπτικά βελτιώνοντας την εκτίμηση του $V(s)$\n",
    "- Αρχικοποίηση: τυχαίες αυθαίρετες τιμές για $V(s)$\n",
    "- Επανάληψη: ανανέωση $Q(s,a)$ και $V(s)$ έως σύγκλιση\n",
    "- Σύγκλιση: εγγυημένη στις βέλτιστες τιμές\n",
    "\n",
    "Ψευδοκώδικας αλγορίθμου:\n",
    "\n",
    "- Αρχικοποίηση $V(s)$ με τυχαίες αυθαίρετες τιμές, πχ $V(s)=0$ για κάθε $s \\in S$\n",
    "- Επανάληψη:\n",
    "    - Για κάθε state $s \\in S$:\n",
    "        - Για κάθε action $a \\in A$:\n",
    "            - $Q(s,a) \\leftarrow E[r|s,a] + \\gamma \\sum_{s' \\in S} p(s'|s,a) \\cdot V(s')$ και όταν το reward δεν εξαρτάται από το action αλλά μόνο από το state: $Q(s,a) \\leftarrow R(s) + \\gamma \\sum_{s' \\in S} p(s'|s,a) \\cdot V(s')$,\n",
    "        - $ V(s) \\leftarrow \\max_{a} Q(s,a)$\n",
    "- Έως ότου $V(s)$ συγκλίνει"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Policy iteration algorithm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While value-iteration algorithm keeps improving the value function at each iteration until the value-function converges. Since the agent only cares about the finding the optimal policy, sometimes the optimal policy will converge before the value function. Therefore, another algorithm called policy-iteration instead of repeated improving the value-function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. Policy iteration is also guaranteed to converge to the optimal policy and it often takes less iterations to converge than the value-iteration algorithm.\n",
    "\n",
    "Ο value iteration αλγόριθμος βελτιώνει σε κάθε iteration την εκτίμηση του value function $V(s)$ έως συγκλίσεως σε μια βέλτιστη τιμή απ' όπου προκύπτει η βέλτιστη πολιτική/policy. Ωστόσο, η βέλτιστη policy ενδέχεται να συγκλίνει πριν τη value function. Επομένως, ο αλγόριθμος policy-iteration αντί να βελτιώνει επαναληπτικά την εκτίμηση του value function. επαναπροσδιορίζει το policy σε κάθε βήμα και υπολογίζει το value με βάση την ανανεωμένη policy έως ότου να συγκλίνει η policy. Ο αλγόριθμος policy iteration εγγυάται σύγκλιση στη βέλτιστη πολιτική και συνήθως απαειτεί λιγότερα βήματα/επαναλήψεις για τη σύγκλιση σε σχέση με τον αλγόριθμο value iteration.\n",
    "\n",
    "**Policy Iteration**\n",
    "- Υπολογίζει: βέλτιστο policy $\\pi$\n",
    "- Πώς: επαναληπτικά βελτιώντας το policy $\\pi$\n",
    "- Αρχικοποίηση: τυχαία αρχικοποίηση για $\\pi$\n",
    "- Επανάληψη: ανανέωση  πολιψυ $\\pi$\n",
    "- Σύγκλιση: εγγυημένη στο βέλτιστο policy\n",
    "\n",
    "Ψευδοκώδικας αλγορίθμου:\n",
    "\n",
    "- Αυθαίρετη αρχικοποίηση $\\pi$\n",
    "- Επανάληψη:\n",
    "    - Υπολογισμός value function $V(s)$ μέσω της επίλυσης των γραμμικών εξισώσεων $V^{\\pi}(s) = E[r|s,\\pi(s)] + \\gamma \\sum_{s' \\in S} p(s'|s,\\pi(s)) \\cdot V^{\\pi}(s')$.\n",
    "    - Βελτίωση του policy σε κάθε state s: $\\pi(s) \\leftarrow \\arg \\max _{a}\\left(E[r \\mid s, a]+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) V^{\\pi}\\left(s^{\\prime}\\right)\\right)$\n",
    "- Έως ότου δεν υπάρχει ανανέωση στο policy $\\pi$, άρα υπάρχει σύγκλιση"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question 4\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Όπως βλέπουμε από το runtime των δύο code blocks για τον value iteration algorithm και τον policy iteration algorithm, ο policy iteration χρειάζεται πολύ λιγότερα iterations για σύκγλιση στη βέλτιστη λύση για το βέλτιστο policy. Ωστόσο ο χρόνος εκτέλεσης για τα δύο code blocks είναι συγκρίσιμος καθώς ο αλγόριθμος είναι πιο περίπλοκος.\n",
    "\n",
    "Η κύρια διαφορά έγκειται στο γεγονός πως ο Value Iteration αλγόριθμος, αν υπάρχουν N πιθανά actions, υπολογίζει το Bellman equation N φορές για κάθε state και να λάβει το max έπειτα, ενώ ο Policy Iteration αλγόριθμος το υπολογίζει μία φορά μόνο για το action που υπαγορεύεται από το τρέχων policy. Ωστόσο, στον Policy Iteration αλγόριθμο, το policy improvement βήμα χρησιμοποιεί ένα max operation το οποίο είναι όσο αργό είναι ένα βήμα στο Value Iteration, αλλά ωστόσο εφόσον ο Policy Iteration αλγόριθμος συγκλίνει σε λιγότερες επαναλήψεις, αυτό το βήμα δε θα συμβεί τόσο συχνά όσο το αντίστοιχο βήμα στον Value Iteration οδηγώντας συνήθως σε λίγο πιο γρήγορη εκτέλεση, όπως συνέβη και στην εν λόγω υπό εξέταση περίπτωση.\n",
    "\n",
    "Στην παρακάτω εικόνα, συνοψίζονται οι δύο διαφορές των αλγορίθμων value και policy iteration:\n",
    "\n",
    "<img src=\"value_iteration_policy_iteration_comparison.jpg\">"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6mci5P4HJ_1",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_43MjfJ9G8v7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(scores))"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-24 18:44:24,884] Making new env: FrozenLake8x8-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy-Iteration converged at step 7.\n",
      "Average scores =  1.0\n",
      "CPU times: total: 3.28 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcikBq6BHRQM",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gHvcnTDcHGmH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print('Policy average score = ', policy_score)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-05-24 18:44:09,701] Making new env: FrozenLake8x8-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at iteration# 2357.\n",
      "Policy average score =  1.0\n",
      "CPU times: total: 4.91 s\n",
      "Wall time: 5.97 s\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Documentation / information sources\n",
    "\n",
    "- <a href=\"https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa\">Value and policy iteration for the Frozen Lake problem</a>\n",
    "- <a href=\"https://www.baeldung.com/cs/ml-value-iteration-vs-policy-iteration\">Value vs policy iteration, comparison</a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}