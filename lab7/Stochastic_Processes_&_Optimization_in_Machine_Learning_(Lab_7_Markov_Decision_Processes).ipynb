{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Stochastic_Processes_&_Optimization_in_Machine_Learning_(Lab_7_Markov_Decision_Processes).ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLZdEbAy2jfr",
    "colab_type": "text"
   },
   "source": [
    "<h1><b>Markov Decision Processes</h1></b>\n",
    "<p align=\"justify\">Στη συγκεκριμένη άσκηση θα μελετήσετε τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, καθώς και θα εξοικειωθείτε με βασικές έννοιες των <i>Markov Decision Processes</i>. Οι αλγόριθμοι <i>Policy Iteration</i> και <i>Value Iteration</i> είναι από τους βασικούς αλγορίθμους δυναμικού προγραμματισμού που χρησιμοποιούνται για την επίλυση της εξίσωσης <i>Bellman</i> σε <i>Markov Decision Processes</i>.</p> \n",
    "<p align=\"justify\">Το πρόβλημα που θα μελετήσετε είναι αυτό της παγωμένης λίμνης (Frozen Lake) με μέγεθος πλέγματος 8 x 8.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VsUV229__zO",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Εξοικείωση με τη βιβλιοθήκη <i>Gym</i></b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OM8ivgOJAg_H",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_puV3ugeAnbU",
    "colab_type": "text"
   },
   "source": [
    "Με την παρακάτω εντολή, ορίζετε το πρόβλημα που θα μελετηθεί:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ep-MvIUCAxT8",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "env_name = 'FrozenLake8x8-v0'\n",
    "env = gym.make(env_name)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "ename": "DeprecatedEnv",
     "evalue": "Environment version `v0` for `FrozenLake8x8` is deprecated. Please use the latest version `v1`.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mDeprecatedEnv\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[1;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m env_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFrozenLake8x8-v0\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 2\u001B[0m env \u001B[38;5;241m=\u001B[39m \u001B[43mgym\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_name\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\gym\\envs\\registration.py:676\u001B[0m, in \u001B[0;36mmake\u001B[1;34m(id, **kwargs)\u001B[0m\n\u001B[0;32m    675\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake\u001B[39m(\u001B[38;5;28mid\u001B[39m: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnv\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 676\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m registry\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;28mid\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\gym\\envs\\registration.py:518\u001B[0m, in \u001B[0;36mEnvRegistry.make\u001B[1;34m(self, path, **kwargs)\u001B[0m\n\u001B[0;32m    515\u001B[0m     path \u001B[38;5;241m=\u001B[39m latest_versioned_spec\u001B[38;5;241m.\u001B[39mid\n\u001B[0;32m    517\u001B[0m \u001B[38;5;66;03m# Lookup our path\u001B[39;00m\n\u001B[1;32m--> 518\u001B[0m spec \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;66;03m# Construct the environment\u001B[39;00m\n\u001B[0;32m    520\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m spec\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\gym\\envs\\registration.py:540\u001B[0m, in \u001B[0;36mEnvRegistry.spec\u001B[1;34m(self, path)\u001B[0m\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28mid\u001B[39m \u001B[38;5;241m=\u001B[39m path\n\u001B[0;32m    538\u001B[0m \u001B[38;5;66;03m# We can go ahead and return the env_spec.\u001B[39;00m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;66;03m# The EnvSpecTree will take care of any exceptions.\u001B[39;00m\n\u001B[1;32m--> 540\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv_specs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mid\u001B[39;49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\gym\\envs\\registration.py:376\u001B[0m, in \u001B[0;36mEnvSpecTree.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m    371\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, key: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m EnvSpec:\n\u001B[0;32m    372\u001B[0m     \u001B[38;5;66;03m# Get an item from the tree.\u001B[39;00m\n\u001B[0;32m    373\u001B[0m     \u001B[38;5;66;03m# We first parse the components so we can look up the\u001B[39;00m\n\u001B[0;32m    374\u001B[0m     \u001B[38;5;66;03m# appropriate environment ID.\u001B[39;00m\n\u001B[0;32m    375\u001B[0m     namespace, name, version \u001B[38;5;241m=\u001B[39m parse_env_id(key)\n\u001B[1;32m--> 376\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_assert_version_exists\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnamespace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mversion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    378\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtree[namespace][name][version]\n",
      "File \u001B[1;32m~\\Downloads\\DSML MsC\\2nd semester\\Στοχαστικές Διεργασίες και Βελτιστοποίηση\\StochasticsLabPublic\\venv\\lib\\site-packages\\gym\\envs\\registration.py:342\u001B[0m, in \u001B[0;36mEnvSpecTree._assert_version_exists\u001B[1;34m(self, namespace, name, version)\u001B[0m\n\u001B[0;32m    340\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis deprecated. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    341\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease use the latest version `v\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlatest_spec\u001B[38;5;241m.\u001B[39mversion\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 342\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\u001B[38;5;241m.\u001B[39mDeprecatedEnv(message)\n\u001B[0;32m    343\u001B[0m \u001B[38;5;66;03m# If this version doesn't exist and there only exists a default version\u001B[39;00m\n\u001B[0;32m    344\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m latest_spec \u001B[38;5;129;01mand\u001B[39;00m latest_spec\u001B[38;5;241m.\u001B[39mversion \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mDeprecatedEnv\u001B[0m: Environment version `v0` for `FrozenLake8x8` is deprecated. Please use the latest version `v1`."
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBKBXJDUBRUh",
    "colab_type": "text"
   },
   "source": [
    "Με τις παρακάτω εντολές, θα επαναφέρετε τον Agent στην αρχική του θέση και θα οπτικοποιήσετε το πλέγμα και τη θέση του Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p6lqbG9zBgdi",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "env.reset()\n",
    "env.render()"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43menv\u001B[49m\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m      2\u001B[0m env\u001B[38;5;241m.\u001B[39mrender()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'env' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX2res4JBlYb",
    "colab_type": "text"
   },
   "source": [
    "Με τις παρακάτω εντολές, ορίζετε την επόμενη ενέργεια με τυχαίο τρόπο και ο Agent κάνει ένα βήμα."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Gq7q944YBx0Q",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "next_action = env.action_space.sample()\n",
    "env.step(next_action)\n",
    "env.render()"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV4A7lsLB54y",
    "colab_type": "text"
   },
   "source": [
    "Να εκτελέσετε αρκετές φορές τις τελευταίες εντολές και να παρατηρήσετε κάθε φορά την ενέργεια που ζητείται από τον agent να εκτελέσει και την ενέργεια που αυτός πραγματοποιεί. Πραγματοποιεί πάντοτε ο agent την κίνηση που του ζητείται; Είναι ντετερμινιστικές ή στοχαστικές οι κινήσεις του agent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAL4we3gDV_J",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Ερωτήσεις</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQKm4VAUChi1",
    "colab_type": "text"
   },
   "source": [
    "<ul>\n",
    "<li>Μελετώντας <a href=\"https://machinelearningjourney.com/index.php/2020/07/02/frozenlake/\">αυτό</a> και βασισμένοι στα συμπεράσματα του προηγούμενου ερωτήματος να περιγράψετε σύντομα το πρόβλημα της παγωμένης λίμνης ως πρόβλημα βελτιστοποίησης. Ποιος είναι ο στόχος του agent;</li>\n",
    "<li>Να διατυπώσετε την ιδιότητα <i>Markov</i>. Πώς απλοποιεί η ιδιότητα αυτή τη μελέτη του συγκεκριμένου προβλήματος;</li>\n",
    "<li>Να περιγράψετε σύντομα τους αλγορίθμους <i>Policy Iteration</i> και <i>Value Iteration</i>, δίνοντας έμφαση στο διαφορετικό τρόπο με τον οποίο προσεγγίζουν την επίλυση του προβλήματος. Είναι εγγυημένο ότι οι δύο αλγόριθμοι θα συγκλίνουν στη βέλτιστη πολιτική; Αν ναι, οδηγούν σε ίδια ή διαφορετική βέλτιστη πολιτική;</li>\n",
    "<li>Να εκτελέσετε τα προγράμματα που σας δίνονται, τα οποία επιλύουν το\n",
    "πρόβλημα της παγωμένης λίμνης, χρησιμοποιώντας τους αλγορίθμους <i>Policy\n",
    "Iteration</i> και <i>Value Iteration</i> αντίστοιχα. Ποια μέθοδος συγκλίνει στη βέλτιστη λύση σε λιγότερα βήματα; Τι συμπέρασμα βγάζετε; Να ελέγξετε το χρόνο εκτέλεσης του κάθε προγράμματος, χρησιμοποιώντας την εντολή <i>time</i>. Τι συμπέρασμα βγάζετε ως προς την πολυπλοκότητα του κάθε αλγορίθμου;</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6mci5P4HJ_1",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Policy Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_43MjfJ9G8v7",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Policy iteration.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Runs an episode and return the total reward \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0, n = 100):\n",
    "    scores = [run_episode(env, policy, gamma, False) for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) for p, s_, r, _ in  env.P[s][a]])\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "def compute_policy_v(env, policy, gamma=1.0):\n",
    "    \"\"\" Iteratively evaluate the value-function under policy.\n",
    "    Alternatively, we could formulate a set of linear equations in iterms of v[s] \n",
    "    and solve them to find the value function.\n",
    "    \"\"\"\n",
    "    v = np.zeros(env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            policy_a = policy[s]\n",
    "            v[s] = sum([p * (r + gamma * prev_v[s_]) for p, s_, r, _ in env.P[s][policy_a]])\n",
    "        if (np.sum((np.fabs(prev_v - v))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return v\n",
    "\n",
    "def policy_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Policy-Iteration algorithm \"\"\"\n",
    "    policy = np.random.choice(env.nA, size=(env.nS))  # initialize a random policy\n",
    "    max_iterations = 200000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_v = compute_policy_v(env, policy, gamma)\n",
    "        new_policy = extract_policy(old_policy_v, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print ('Policy-Iteration converged at step %d.' %(i+1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_policy = policy_iteration(env, gamma = 1.0)\n",
    "    scores = evaluate_policy(env, optimal_policy, gamma = 1.0)\n",
    "    print('Average scores = ', np.mean(scores))"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcikBq6BHRQM",
    "colab_type": "text"
   },
   "source": [
    "<h2><b>Value Iteration</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gHvcnTDcHGmH",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\"\"\"\n",
    "Solving FrozenLake8x8 environment using Value-Itertion.\n",
    "Author : Moustafa Alzantot (malzantot@ucla.edu)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "\n",
    "def run_episode(env, policy, gamma = 1.0, render = False):\n",
    "    \"\"\" Evaluates policy by using it to run an episode and finding its\n",
    "    total reward.\n",
    "    args:\n",
    "    env: gym environment.\n",
    "    policy: the policy to be used.\n",
    "    gamma: discount factor.\n",
    "    render: boolean to turn rendering on/off.\n",
    "    returns:\n",
    "    total reward: real value of the total reward recieved by agent under policy.\n",
    "    \"\"\"\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    step_idx = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        obs, reward, done , _ = env.step(int(policy[obs]))\n",
    "        total_reward += (gamma ** step_idx * reward)\n",
    "        step_idx += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, gamma = 1.0,  n = 100):\n",
    "    \"\"\" Evaluates a policy by running it n times.\n",
    "    returns:\n",
    "    average total reward\n",
    "    \"\"\"\n",
    "    scores = [\n",
    "            run_episode(env, policy, gamma = gamma, render = False)\n",
    "            for _ in range(n)]\n",
    "    return np.mean(scores)\n",
    "\n",
    "def extract_policy(v, gamma = 1.0):\n",
    "    \"\"\" Extract the policy given a value-function \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for s in range(env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "def value_iteration(env, gamma = 1.0):\n",
    "    \"\"\" Value-iteration algorithm \"\"\"\n",
    "    v = np.zeros(env.nS)  # initialize value-function\n",
    "    max_iterations = 100000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        prev_v = np.copy(v)\n",
    "        for s in range(env.nS):\n",
    "            q_sa = [sum([p*(r + prev_v[s_]) for p, s_, r, _ in env.P[s][a]]) for a in range(env.nA)] \n",
    "            v[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(prev_v - v)) <= eps):\n",
    "            print ('Value-iteration converged at iteration# %d.' %(i+1))\n",
    "            break\n",
    "    return v\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env_name  = 'FrozenLake8x8-v0'\n",
    "    gamma = 1.0\n",
    "    env = gym.make(env_name)\n",
    "    env = env.unwrapped\n",
    "    optimal_v = value_iteration(env, gamma);\n",
    "    policy = extract_policy(optimal_v, gamma)\n",
    "    policy_score = evaluate_policy(env, policy, gamma, n=1000)\n",
    "    print('Policy average score = ', policy_score)"
   ],
   "execution_count": 0,
   "outputs": []
  }
 ]
}